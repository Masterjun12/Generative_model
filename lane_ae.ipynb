{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPfx2OapmUJM1JklpdcoFcT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masterjun12/Generative_model/blob/main/lane_ae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgsbf8HKsRPV",
        "outputId": "7eb1f8cf-3f5c-4394-f097-0c2e41bd96b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.26)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.49.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in ld-4 to yolov5-obb:: 100%|██████████| 36913/36913 [00:03<00:00, 11679.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to ld-4 in yolov5-obb:: 100%|██████████| 1650/1650 [00:00<00:00, 7120.30it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"tC8ti52DSK7wGjAdfw1p\")\n",
        "project = rf.workspace(\"smt-indira-gandhi-college-of-engineering-iojgu\").project(\"ld-le5n7\")\n",
        "version = project.version(4)\n",
        "dataset = version.download(\"yolov5-obb\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ECjcaa9JZK8",
        "outputId": "829a6a36-a8df-4228-b231-c459fba8d149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_dir = \"/content/gdrive/MyDrive/notebooks/tests\"\n",
        "target_dir = \"/content/tests\"\n",
        "\n",
        "shutil.copytree(source_dir, target_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9ixNAVRrHopU",
        "outputId": "871d2c8f-5118-4393-e5c0-7e8939bca810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/tests'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def parse_segmentation_info(txt_file_path):\n",
        "    \"\"\"\n",
        "    Parses the segmentation information from the text file.\n",
        "    Assumes the segmentation info is in a specific format.\n",
        "    You might need to adjust this function based on your actual data format.\n",
        "    \"\"\"\n",
        "    segmentation_info = []\n",
        "    with open(txt_file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Splitting by space assuming the format is like: x1 y1 x2 y2 class\n",
        "            # Parsing only the first 8 elements as coordinates (ignoring class)\n",
        "            coordinates = [float(x) for x in line.strip().split()[:8]]\n",
        "            segmentation_info.append(coordinates)\n",
        "    return segmentation_info\n",
        "\n",
        "def crop_and_save(image_path, segmentation_info, output_folder):\n",
        "    \"\"\"\n",
        "    Crops the image based on the segmentation information and saves it to the output folder.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "    for i, seg_info in enumerate(segmentation_info):\n",
        "        # Extracting coordinates\n",
        "        x1, y1, x2, y2, x3, y3, x4, y4 = seg_info\n",
        "        # Finding minimum and maximum coordinates for the bounding box\n",
        "        min_x = min(x1, x2, x3, x4)\n",
        "        min_y = min(y1, y2, y3, y4)\n",
        "        max_x = max(x1, x2, x3, x4)\n",
        "        max_y = max(y1, y2, y3, y4)\n",
        "        # Cropping the region from the image\n",
        "        cropped_region = image.crop((min_x, min_y, max_x, max_y))\n",
        "        # Saving the cropped region to the output folder\n",
        "        cropped_region.save(os.path.join(output_folder, f\"{image_name}_crop_{i}.png\"))\n",
        "\n",
        "def process_dataset(image_folder, label_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Processes the dataset by reading images and their corresponding segmentation info,\n",
        "    cropping the images based on the segmentation info, and saving the cropped images\n",
        "    to the output folder.\n",
        "    \"\"\"\n",
        "    # Ensure the output folder exists, create if not\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # List all image files in the image folder\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Get the corresponding text file\n",
        "        txt_file = os.path.join(label_folder, image_file.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "        if os.path.exists(txt_file):\n",
        "            # Parse segmentation info from the text file\n",
        "            segmentation_info = parse_segmentation_info(txt_file)\n",
        "            # Crop and save the image based on segmentation info\n",
        "            crop_and_save(os.path.join(image_folder, image_file), segmentation_info, output_folder)\n",
        "\n",
        "# Example usage:\n",
        "image_folder = \"/content/gdrive/MyDrive/notebooks/train/images\"\n",
        "label_folder = \"/content/gdrive/MyDrive/notebooks/train/labelTxt\"\n",
        "output_folder = \"/content/gdrive/MyDrive/notebooks/crop\"\n",
        "\n",
        "process_dataset(image_folder, label_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "AI7j8Ud_tOEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def split_train_test_images(input_folder, train_folder, test_folder, test_ratio=0.2):\n",
        "    # Ensure the output folders exist, create if not\n",
        "    os.makedirs(train_folder, exist_ok=True)\n",
        "    os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "    # List all image files in the input folder\n",
        "    image_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
        "\n",
        "    # Shuffle the image files\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    # Calculate the number of images for the test set\n",
        "    num_test_images = int(len(image_files) * test_ratio)\n",
        "\n",
        "    # Split the image files into train and test sets\n",
        "    train_images = image_files[:-num_test_images]\n",
        "    test_images = image_files[-num_test_images:]\n",
        "\n",
        "    # Copy train images to the train folder\n",
        "    for image in train_images:\n",
        "        shutil.copyfile(os.path.join(input_folder, image), os.path.join(train_folder, image))\n",
        "\n",
        "    # Copy test images to the test folder\n",
        "    for image in test_images:\n",
        "        shutil.copyfile(os.path.join(input_folder, image), os.path.join(test_folder, image))\n",
        "\n",
        "# Paths\n",
        "input_folder = \"/content/gdrive/MyDrive/notebooks/crop\"\n",
        "train_folder = \"/content/gdrive/MyDrive/notebooks/trains\"\n",
        "test_folder = \"/content/gdrive/MyDrive/notebooks/tests\"\n",
        "\n",
        "# Split train and test images\n",
        "split_train_test_images(input_folder, train_folder, test_folder, test_ratio=0.2)\n"
      ],
      "metadata": {
        "id": "vuV2w9GlBYcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/ld-4') # 경로삭제"
      ],
      "metadata": {
        "id": "UFozJNcgvKMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def otsu_binarization(image):\n",
        "    \"\"\"\n",
        "    Applies OTSU binarization to the input image.\n",
        "    \"\"\"\n",
        "    # Convert image to numpy array\n",
        "    np_image = np.array(image)\n",
        "    # Convert image to grayscale\n",
        "    grayscale_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2GRAY)\n",
        "    # Apply OTSU thresholding to the grayscale image\n",
        "    _, binary_image = cv2.threshold(grayscale_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    return Image.fromarray(binary_image)\n",
        "\n",
        "def process_images_in_folder(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Processes all PNG images in the input folder using OTSU binarization\n",
        "    and saves the binarized images to the output folder.\n",
        "    \"\"\"\n",
        "    # Ensure the output folder exists, create if not\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # List all PNG files in the input folder\n",
        "    image_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Open and convert the image file to RGB mode\n",
        "        image_path = os.path.join(input_folder, image_file)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        # Apply OTSU binarization\n",
        "        binary_image = otsu_binarization(image)\n",
        "        # Save the binarized image to the output folder\n",
        "        output_path = os.path.join(output_folder, image_file)\n",
        "        binary_image.save(output_path)\n",
        "\n",
        "# Example usage:\n",
        "input_folder = \"/content/ld-4/train/train\"\n",
        "output_folder = \"/content/ld-4/train/train_otsu\"\n",
        "\n",
        "process_images_in_folder(input_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "D5Q0ZXwpuZhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall opencv-python-headless -y\n",
        "!pip uninstall opencv-contrib-python-headless -y\n",
        "!pip install opencv-python-headless\n",
        "!pip install opencv-contrib-python-headless\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "ge8Et0xzaCQ2",
        "outputId": "abaa6c7e-491c-4148-a04e-046e6d59ee73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opencv-python-headless 4.8.0.74\n",
            "Uninstalling opencv-python-headless-4.8.0.74:\n",
            "  Successfully uninstalled opencv-python-headless-4.8.0.74\n",
            "\u001b[33mWARNING: Skipping opencv-contrib-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n",
            "Installing collected packages: opencv-python-headless\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "roboflow 1.1.26 requires opencv-python-headless==4.8.0.74, but you have opencv-python-headless 4.9.0.80 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed opencv-python-headless-4.9.0.80\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              },
              "id": "85e039e4a0c44e53b02f04e76ece1abb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-contrib-python-headless\n",
            "  Downloading opencv_contrib_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (55.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python-headless) (1.25.2)\n",
            "Installing collected packages: opencv-contrib-python-headless\n",
            "Successfully installed opencv-contrib-python-headless-4.9.0.80\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              },
              "id": "4b7a05b57171483bb3b8b90a6234bc59"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfBY2FIokrr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from PIL import Image\n",
        "import shutil\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def create_autoencoder(input_shape):\n",
        "    input_img = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary_image = cv2.threshold(image.astype('uint8'), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    resized_image = cv2.resize(binary_image, (128, 128))\n",
        "    normalized_image = resized_image.astype('float32') / 255.0\n",
        "    final_image = np.reshape(normalized_image, (128, 128, 1))\n",
        "\n",
        "    return final_image\n",
        "\n",
        "\n",
        "# Function to train the Autoencoder\n",
        "def train_autoencoder(input_folder):\n",
        "    # Load and preprocess images for training\n",
        "    image_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
        "    images = np.array([preprocess_image(f) for f in image_files])\n",
        "\n",
        "    # Train the Autoencoder\n",
        "\n",
        "    checkpoint = ModelCheckpoint('autoencoder_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "    autoencoder.fit(images, images, epochs=200 , batch_size=32, shuffle=True, validation_split=0.2, callbacks=[checkpoint])\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "# Train the Autoencoder\n",
        "input_folder = \"/content/trains\"\n",
        "autoencoder = train_autoencoder(input_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXC-6IKa6we_",
        "outputId": "5d3bf3d9-ddf6-41a8-9b51-a7a7522f6ab4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0505\n",
            "Epoch 1: val_loss improved from inf to 0.05219, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 1s 11ms/step - loss: 0.0507 - val_loss: 0.0522\n",
            "Epoch 2/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0507\n",
            "Epoch 2: val_loss did not improve from 0.05219\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0523\n",
            "Epoch 3/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 3: val_loss did not improve from 0.05219\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0522\n",
            "Epoch 4/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0511\n",
            "Epoch 4: val_loss did not improve from 0.05219\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0512 - val_loss: 0.0524\n",
            "Epoch 5/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 5: val_loss improved from 0.05219 to 0.05217, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0508 - val_loss: 0.0522\n",
            "Epoch 6/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 6: val_loss did not improve from 0.05217\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0525\n",
            "Epoch 7/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0506\n",
            "Epoch 7: val_loss did not improve from 0.05217\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0522\n",
            "Epoch 8/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 8: val_loss did not improve from 0.05217\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0525\n",
            "Epoch 9/200\n",
            "45/50 [==========================>...] - ETA: 0s - loss: 0.0507\n",
            "Epoch 9: val_loss improved from 0.05217 to 0.05211, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0507 - val_loss: 0.0521\n",
            "Epoch 10/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0506\n",
            "Epoch 10: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0522\n",
            "Epoch 11/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0505\n",
            "Epoch 11: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0522\n",
            "Epoch 12/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 12: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0521\n",
            "Epoch 13/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0510\n",
            "Epoch 13: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0510 - val_loss: 0.0527\n",
            "Epoch 14/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0507\n",
            "Epoch 14: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0523\n",
            "Epoch 15/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 15: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0521\n",
            "Epoch 16/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0508\n",
            "Epoch 16: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0521\n",
            "Epoch 17/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0508\n",
            "Epoch 17: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0522\n",
            "Epoch 18/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 18: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0522\n",
            "Epoch 19/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0506\n",
            "Epoch 19: val_loss did not improve from 0.05211\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0522\n",
            "Epoch 20/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 20: val_loss improved from 0.05211 to 0.05204, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0520\n",
            "Epoch 21/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 21: val_loss did not improve from 0.05204\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0521\n",
            "Epoch 22/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 22: val_loss did not improve from 0.05204\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0522\n",
            "Epoch 23/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 23: val_loss did not improve from 0.05204\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0509 - val_loss: 0.0522\n",
            "Epoch 24/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0506\n",
            "Epoch 24: val_loss did not improve from 0.05204\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0521\n",
            "Epoch 25/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 25: val_loss did not improve from 0.05204\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0521\n",
            "Epoch 26/200\n",
            "42/50 [========================>.....] - ETA: 0s - loss: 0.0506\n",
            "Epoch 26: val_loss improved from 0.05204 to 0.05203, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0520\n",
            "Epoch 27/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 27: val_loss did not improve from 0.05203\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0522\n",
            "Epoch 28/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0506\n",
            "Epoch 28: val_loss improved from 0.05203 to 0.05201, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0506 - val_loss: 0.0520\n",
            "Epoch 29/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 29: val_loss did not improve from 0.05201\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0509 - val_loss: 0.0540\n",
            "Epoch 30/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 30: val_loss did not improve from 0.05201\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0509 - val_loss: 0.0521\n",
            "Epoch 31/200\n",
            "42/50 [========================>.....] - ETA: 0s - loss: 0.0508\n",
            "Epoch 31: val_loss did not improve from 0.05201\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0520\n",
            "Epoch 32/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 32: val_loss did not improve from 0.05201\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0521\n",
            "Epoch 33/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 33: val_loss did not improve from 0.05201\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0520\n",
            "Epoch 34/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0507\n",
            "Epoch 34: val_loss did not improve from 0.05201\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0524\n",
            "Epoch 35/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0508\n",
            "Epoch 35: val_loss did not improve from 0.05201\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0523\n",
            "Epoch 36/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0510\n",
            "Epoch 36: val_loss did not improve from 0.05201\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0511 - val_loss: 0.0527\n",
            "Epoch 37/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 37: val_loss improved from 0.05201 to 0.05197, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0520\n",
            "Epoch 38/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 38: val_loss did not improve from 0.05197\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0520\n",
            "Epoch 39/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0509\n",
            "Epoch 39: val_loss did not improve from 0.05197\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0509 - val_loss: 0.0521\n",
            "Epoch 40/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0505\n",
            "Epoch 40: val_loss did not improve from 0.05197\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0520\n",
            "Epoch 41/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 41: val_loss improved from 0.05197 to 0.05195, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 42/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 42: val_loss did not improve from 0.05195\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0524\n",
            "Epoch 43/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0507\n",
            "Epoch 43: val_loss improved from 0.05195 to 0.05194, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0507 - val_loss: 0.0519\n",
            "Epoch 44/200\n",
            "46/50 [==========================>...] - ETA: 0s - loss: 0.0500\n",
            "Epoch 44: val_loss did not improve from 0.05194\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0520\n",
            "Epoch 45/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0508\n",
            "Epoch 45: val_loss improved from 0.05194 to 0.05192, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0506 - val_loss: 0.0519\n",
            "Epoch 46/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0504\n",
            "Epoch 46: val_loss did not improve from 0.05192\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0538\n",
            "Epoch 47/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0508\n",
            "Epoch 47: val_loss did not improve from 0.05192\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0521\n",
            "Epoch 48/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0504\n",
            "Epoch 48: val_loss did not improve from 0.05192\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0520\n",
            "Epoch 49/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 49: val_loss did not improve from 0.05192\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 50/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 50: val_loss improved from 0.05192 to 0.05189, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 51/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 51: val_loss did not improve from 0.05189\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0520\n",
            "Epoch 52/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 52: val_loss did not improve from 0.05189\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0521\n",
            "Epoch 53/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0511\n",
            "Epoch 53: val_loss did not improve from 0.05189\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0511 - val_loss: 0.0520\n",
            "Epoch 54/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 54: val_loss did not improve from 0.05189\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 55/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 55: val_loss did not improve from 0.05189\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0522\n",
            "Epoch 56/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 56: val_loss improved from 0.05189 to 0.05188, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 57/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 57: val_loss improved from 0.05188 to 0.05186, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0519\n",
            "Epoch 58/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 58: val_loss did not improve from 0.05186\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0521\n",
            "Epoch 59/200\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0505\n",
            "Epoch 59: val_loss did not improve from 0.05186\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0520\n",
            "Epoch 60/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 60: val_loss improved from 0.05186 to 0.05185, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 61/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0503\n",
            "Epoch 61: val_loss did not improve from 0.05185\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0520\n",
            "Epoch 62/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 62: val_loss did not improve from 0.05185\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0519\n",
            "Epoch 63/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0506\n",
            "Epoch 63: val_loss did not improve from 0.05185\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 64/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 64: val_loss did not improve from 0.05185\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0522\n",
            "Epoch 65/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0507\n",
            "Epoch 65: val_loss did not improve from 0.05185\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0533\n",
            "Epoch 66/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 66: val_loss did not improve from 0.05185\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0519\n",
            "Epoch 67/200\n",
            "42/50 [========================>.....] - ETA: 0s - loss: 0.0505\n",
            "Epoch 67: val_loss did not improve from 0.05185\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 68/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 68: val_loss improved from 0.05185 to 0.05179, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0506 - val_loss: 0.0518\n",
            "Epoch 69/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 69: val_loss improved from 0.05179 to 0.05178, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0518\n",
            "Epoch 70/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 70: val_loss did not improve from 0.05178\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0518\n",
            "Epoch 71/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 71: val_loss improved from 0.05178 to 0.05177, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0518\n",
            "Epoch 72/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0511\n",
            "Epoch 72: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0518\n",
            "Epoch 73/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 73: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0518\n",
            "Epoch 74/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 74: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 0.0539\n",
            "Epoch 75/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0506\n",
            "Epoch 75: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0519\n",
            "Epoch 76/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0505\n",
            "Epoch 76: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0521\n",
            "Epoch 77/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0505\n",
            "Epoch 77: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0518\n",
            "Epoch 78/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0508\n",
            "Epoch 78: val_loss improved from 0.05177 to 0.05177, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0518\n",
            "Epoch 79/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 79: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0521\n",
            "Epoch 80/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 80: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0525\n",
            "Epoch 81/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 81: val_loss improved from 0.05177 to 0.05177, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0518\n",
            "Epoch 82/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 82: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0521\n",
            "Epoch 83/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 83: val_loss did not improve from 0.05177\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 84/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 84: val_loss improved from 0.05177 to 0.05175, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0503 - val_loss: 0.0518\n",
            "Epoch 85/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 85: val_loss did not improve from 0.05175\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0519\n",
            "Epoch 86/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0502\n",
            "Epoch 86: val_loss did not improve from 0.05175\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0518\n",
            "Epoch 87/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 87: val_loss improved from 0.05175 to 0.05173, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0517\n",
            "Epoch 88/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 88: val_loss did not improve from 0.05173\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0521\n",
            "Epoch 89/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 89: val_loss did not improve from 0.05173\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0519\n",
            "Epoch 90/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 90: val_loss did not improve from 0.05173\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0519\n",
            "Epoch 91/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 91: val_loss improved from 0.05173 to 0.05169, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0503 - val_loss: 0.0517\n",
            "Epoch 92/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 92: val_loss did not improve from 0.05169\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0517\n",
            "Epoch 93/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0504\n",
            "Epoch 93: val_loss improved from 0.05169 to 0.05168, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0503 - val_loss: 0.0517\n",
            "Epoch 94/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 94: val_loss did not improve from 0.05168\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0522\n",
            "Epoch 95/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 95: val_loss did not improve from 0.05168\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0517\n",
            "Epoch 96/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 96: val_loss did not improve from 0.05168\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0519\n",
            "Epoch 97/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0504\n",
            "Epoch 97: val_loss did not improve from 0.05168\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0517\n",
            "Epoch 98/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 98: val_loss did not improve from 0.05168\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 0.0530\n",
            "Epoch 99/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 99: val_loss did not improve from 0.05168\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 0.0524\n",
            "Epoch 100/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0504\n",
            "Epoch 100: val_loss improved from 0.05168 to 0.05164, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0516\n",
            "Epoch 101/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 101: val_loss did not improve from 0.05164\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0518\n",
            "Epoch 102/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 102: val_loss improved from 0.05164 to 0.05163, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0503 - val_loss: 0.0516\n",
            "Epoch 103/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 103: val_loss did not improve from 0.05163\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0518\n",
            "Epoch 104/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 104: val_loss did not improve from 0.05163\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0518\n",
            "Epoch 105/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0504\n",
            "Epoch 105: val_loss improved from 0.05163 to 0.05162, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0516\n",
            "Epoch 106/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 106: val_loss did not improve from 0.05162\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0520\n",
            "Epoch 107/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 107: val_loss did not improve from 0.05162\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0519\n",
            "Epoch 108/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 108: val_loss did not improve from 0.05162\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0516\n",
            "Epoch 109/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 109: val_loss improved from 0.05162 to 0.05158, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0502 - val_loss: 0.0516\n",
            "Epoch 110/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 110: val_loss did not improve from 0.05158\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0516\n",
            "Epoch 111/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 111: val_loss did not improve from 0.05158\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0521\n",
            "Epoch 112/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0506\n",
            "Epoch 112: val_loss did not improve from 0.05158\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0518\n",
            "Epoch 113/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 113: val_loss did not improve from 0.05158\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0517\n",
            "Epoch 114/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 114: val_loss did not improve from 0.05158\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0524\n",
            "Epoch 115/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 115: val_loss did not improve from 0.05158\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0516\n",
            "Epoch 116/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 116: val_loss improved from 0.05158 to 0.05155, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0502 - val_loss: 0.0516\n",
            "Epoch 117/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 117: val_loss did not improve from 0.05155\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0536\n",
            "Epoch 118/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 118: val_loss did not improve from 0.05155\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0521\n",
            "Epoch 119/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 119: val_loss did not improve from 0.05155\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0517\n",
            "Epoch 120/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0501\n",
            "Epoch 120: val_loss did not improve from 0.05155\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0516\n",
            "Epoch 121/200\n",
            "42/50 [========================>.....] - ETA: 0s - loss: 0.0497\n",
            "Epoch 121: val_loss did not improve from 0.05155\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0516\n",
            "Epoch 122/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0498\n",
            "Epoch 122: val_loss did not improve from 0.05155\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0516\n",
            "Epoch 123/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 123: val_loss improved from 0.05155 to 0.05154, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0502 - val_loss: 0.0515\n",
            "Epoch 124/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0499\n",
            "Epoch 124: val_loss did not improve from 0.05154\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0520\n",
            "Epoch 125/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 125: val_loss did not improve from 0.05154\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0516\n",
            "Epoch 126/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 126: val_loss did not improve from 0.05154\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0520\n",
            "Epoch 127/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0499\n",
            "Epoch 127: val_loss improved from 0.05154 to 0.05150, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0501 - val_loss: 0.0515\n",
            "Epoch 128/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 128: val_loss did not improve from 0.05150\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0516\n",
            "Epoch 129/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 129: val_loss did not improve from 0.05150\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0516\n",
            "Epoch 130/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 130: val_loss did not improve from 0.05150\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0516\n",
            "Epoch 131/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0499\n",
            "Epoch 131: val_loss did not improve from 0.05150\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0517\n",
            "Epoch 132/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 132: val_loss improved from 0.05150 to 0.05148, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0504 - val_loss: 0.0515\n",
            "Epoch 133/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 133: val_loss did not improve from 0.05148\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0516\n",
            "Epoch 134/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 134: val_loss did not improve from 0.05148\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0515\n",
            "Epoch 135/200\n",
            "46/50 [==========================>...] - ETA: 0s - loss: 0.0496\n",
            "Epoch 135: val_loss did not improve from 0.05148\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0501 - val_loss: 0.0521\n",
            "Epoch 136/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0505\n",
            "Epoch 136: val_loss did not improve from 0.05148\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0515\n",
            "Epoch 137/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0500\n",
            "Epoch 137: val_loss improved from 0.05148 to 0.05146, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0501 - val_loss: 0.0515\n",
            "Epoch 138/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0501\n",
            "Epoch 138: val_loss did not improve from 0.05146\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0515\n",
            "Epoch 139/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0502\n",
            "Epoch 139: val_loss did not improve from 0.05146\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0502 - val_loss: 0.0515\n",
            "Epoch 140/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 140: val_loss improved from 0.05146 to 0.05143, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0500 - val_loss: 0.0514\n",
            "Epoch 141/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 141: val_loss did not improve from 0.05143\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0514\n",
            "Epoch 142/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 142: val_loss did not improve from 0.05143\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0515\n",
            "Epoch 143/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 143: val_loss did not improve from 0.05143\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0515\n",
            "Epoch 144/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 144: val_loss did not improve from 0.05143\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0516\n",
            "Epoch 145/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 145: val_loss improved from 0.05143 to 0.05141, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0500 - val_loss: 0.0514\n",
            "Epoch 146/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 146: val_loss did not improve from 0.05141\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0514\n",
            "Epoch 147/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0499\n",
            "Epoch 147: val_loss did not improve from 0.05141\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0514\n",
            "Epoch 148/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 148: val_loss did not improve from 0.05141\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0514\n",
            "Epoch 149/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 149: val_loss did not improve from 0.05141\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0532\n",
            "Epoch 150/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0504\n",
            "Epoch 150: val_loss did not improve from 0.05141\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0515\n",
            "Epoch 151/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 151: val_loss did not improve from 0.05141\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0518\n",
            "Epoch 152/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 152: val_loss did not improve from 0.05141\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0515\n",
            "Epoch 153/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 153: val_loss improved from 0.05141 to 0.05138, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0500 - val_loss: 0.0514\n",
            "Epoch 154/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0497\n",
            "Epoch 154: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0515\n",
            "Epoch 155/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 155: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0515\n",
            "Epoch 156/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 156: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0516\n",
            "Epoch 157/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 157: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0514\n",
            "Epoch 158/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 158: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0515\n",
            "Epoch 159/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0506\n",
            "Epoch 159: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 0.0514\n",
            "Epoch 160/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 160: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0529\n",
            "Epoch 161/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0510\n",
            "Epoch 161: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0511 - val_loss: 0.0518\n",
            "Epoch 162/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 162: val_loss did not improve from 0.05138\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0516\n",
            "Epoch 163/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 163: val_loss improved from 0.05138 to 0.05135, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0500 - val_loss: 0.0514\n",
            "Epoch 164/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0499\n",
            "Epoch 164: val_loss did not improve from 0.05135\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0499 - val_loss: 0.0514\n",
            "Epoch 165/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 165: val_loss improved from 0.05135 to 0.05135, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0499 - val_loss: 0.0513\n",
            "Epoch 166/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 166: val_loss did not improve from 0.05135\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0514\n",
            "Epoch 167/200\n",
            "46/50 [==========================>...] - ETA: 0s - loss: 0.0499\n",
            "Epoch 167: val_loss did not improve from 0.05135\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0514\n",
            "Epoch 168/200\n",
            "45/50 [==========================>...] - ETA: 0s - loss: 0.0495\n",
            "Epoch 168: val_loss improved from 0.05135 to 0.05130, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0499 - val_loss: 0.0513\n",
            "Epoch 169/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0498\n",
            "Epoch 169: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0514\n",
            "Epoch 170/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 170: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0513\n",
            "Epoch 171/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 171: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0513\n",
            "Epoch 172/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 172: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0514\n",
            "Epoch 173/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 173: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0513\n",
            "Epoch 174/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0500\n",
            "Epoch 174: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0513\n",
            "Epoch 175/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 175: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0524\n",
            "Epoch 176/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0503\n",
            "Epoch 176: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 0.0530\n",
            "Epoch 177/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 177: val_loss did not improve from 0.05130\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0513\n",
            "Epoch 178/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 178: val_loss improved from 0.05130 to 0.05124, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0499 - val_loss: 0.0512\n",
            "Epoch 179/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0498\n",
            "Epoch 179: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0516\n",
            "Epoch 180/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0500\n",
            "Epoch 180: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 0.0513\n",
            "Epoch 181/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0500\n",
            "Epoch 181: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0516\n",
            "Epoch 182/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 182: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0514\n",
            "Epoch 183/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0498\n",
            "Epoch 183: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0519\n",
            "Epoch 184/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 184: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0513\n",
            "Epoch 185/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 185: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0513\n",
            "Epoch 186/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0499\n",
            "Epoch 186: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0514\n",
            "Epoch 187/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 187: val_loss did not improve from 0.05124\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0513\n",
            "Epoch 188/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0498\n",
            "Epoch 188: val_loss improved from 0.05124 to 0.05123, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0498 - val_loss: 0.0512\n",
            "Epoch 189/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0497\n",
            "Epoch 189: val_loss did not improve from 0.05123\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0498 - val_loss: 0.0520\n",
            "Epoch 190/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0501\n",
            "Epoch 190: val_loss did not improve from 0.05123\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 0.0513\n",
            "Epoch 191/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0499\n",
            "Epoch 191: val_loss improved from 0.05123 to 0.05120, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0498 - val_loss: 0.0512\n",
            "Epoch 192/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0498\n",
            "Epoch 192: val_loss did not improve from 0.05120\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 0.0519\n",
            "Epoch 193/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0502\n",
            "Epoch 193: val_loss improved from 0.05120 to 0.05119, saving model to autoencoder_weights.h5\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0499 - val_loss: 0.0512\n",
            "Epoch 194/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0498\n",
            "Epoch 194: val_loss did not improve from 0.05119\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0498 - val_loss: 0.0514\n",
            "Epoch 195/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0498\n",
            "Epoch 195: val_loss did not improve from 0.05119\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0498 - val_loss: 0.0514\n",
            "Epoch 196/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0502\n",
            "Epoch 196: val_loss did not improve from 0.05119\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 0.0550\n",
            "Epoch 197/200\n",
            "48/50 [===========================>..] - ETA: 0s - loss: 0.0509\n",
            "Epoch 197: val_loss did not improve from 0.05119\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 0.0514\n",
            "Epoch 198/200\n",
            "45/50 [==========================>...] - ETA: 0s - loss: 0.0503\n",
            "Epoch 198: val_loss did not improve from 0.05119\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.0499 - val_loss: 0.0513\n",
            "Epoch 199/200\n",
            "47/50 [===========================>..] - ETA: 0s - loss: 0.0497\n",
            "Epoch 199: val_loss did not improve from 0.05119\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0498 - val_loss: 0.0512\n",
            "Epoch 200/200\n",
            "49/50 [============================>.] - ETA: 0s - loss: 0.0498\n",
            "Epoch 200: val_loss did not improve from 0.05119\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.0498 - val_loss: 0.0513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect anomalies\n",
        "def detect_anomalies(autoencoder, image_path, threshold=0.01):\n",
        "    # Preprocess input image\n",
        "    input_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Convert to uint8\n",
        "    input_image = input_image.astype('uint8')\n",
        "    # Perform OTSU thresholding\n",
        "    _, binary_image = cv2.threshold(input_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    resized_image = cv2.resize(binary_image, (128, 128))\n",
        "    normalized_image = resized_image.astype('float32') / 255.0\n",
        "    final_image = np.reshape(normalized_image, (1, 128, 128, 1))\n",
        "\n",
        "    # Generate reconstruction from Autoencoder\n",
        "    reconstructed_image = autoencoder.predict(final_image)\n",
        "    # Calculate Mean Squared Error (MSE) between input and reconstructed images\n",
        "    mse = np.mean(np.square(final_image - reconstructed_image))\n",
        "\n",
        "    # Determine anomaly based on MSE and threshold\n",
        "    if mse > threshold:\n",
        "        return 1  # Anomaly\n",
        "    else:\n",
        "        return 0  # Normal\n",
        "\n",
        "\n",
        "# Function to evaluate anomalies in a test folder\n",
        "def evaluate_anomalies(autoencoder, test_folder):\n",
        "    labels = []\n",
        "    predictions = []\n",
        "\n",
        "    for subdir in os.listdir(test_folder):\n",
        "        subdir_path = os.path.join(test_folder, subdir)\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            continue\n",
        "\n",
        "        for image_file in os.listdir(subdir_path):\n",
        "            image_path = os.path.join(subdir_path, image_file)\n",
        "            label = int(subdir)  # Use folder name as label\n",
        "            labels.append(label)\n",
        "            prediction = detect_anomalies(autoencoder, image_path)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "    return np.array(labels), np.array(predictions)\n",
        "\n",
        "# Evaluate anomalies in the test folder\n",
        "test_folder = \"/content/tests\"\n",
        "true_labels, predicted_labels = evaluate_anomalies(autoencoder, test_folder)\n",
        "\n",
        "# Calculate F1 score and confusion matrix\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2BddIhmcp_J",
        "outputId": "a68722a8-fa76-4a7f-dc8d-0802d41e30fa"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "F1 Score: 0.163265306122449\n",
            "Confusion Matrix:\n",
            "[[445  12]\n",
            " [ 29   4]]\n"
          ]
        }
      ]
    }
  ]
}