{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1_K0WHpJGPR3Sjfd503KPkYt5lUYZu2Bh",
      "authorship_tag": "ABX9TyMIO89rORIclwLCihnkvl8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masterjun12/Generative_model/blob/main/lane_ae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgsbf8HKsRPV",
        "outputId": "7eb1f8cf-3f5c-4394-f097-0c2e41bd96b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.26)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.49.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in ld-4 to yolov5-obb:: 100%|██████████| 36913/36913 [00:03<00:00, 11679.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to ld-4 in yolov5-obb:: 100%|██████████| 1650/1650 [00:00<00:00, 7120.30it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"tC8ti52DSK7wGjAdfw1p\")\n",
        "project = rf.workspace(\"smt-indira-gandhi-college-of-engineering-iojgu\").project(\"ld-le5n7\")\n",
        "version = project.version(4)\n",
        "dataset = version.download(\"yolov5-obb\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QwTb2ej9sRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ECjcaa9JZK8",
        "outputId": "829a6a36-a8df-4228-b231-c459fba8d149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_dir = \"/content/gdrive/MyDrive/notebooks/tests\"\n",
        "target_dir = \"/content/tests\"\n",
        "\n",
        "shutil.copytree(source_dir, target_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9ixNAVRrHopU",
        "outputId": "871d2c8f-5118-4393-e5c0-7e8939bca810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/tests'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def parse_segmentation_info(txt_file_path):\n",
        "    \"\"\"\n",
        "    Parses the segmentation information from the text file.\n",
        "    Assumes the segmentation info is in a specific format.\n",
        "    You might need to adjust this function based on your actual data format.\n",
        "    \"\"\"\n",
        "    segmentation_info = []\n",
        "    with open(txt_file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Splitting by space assuming the format is like: x1 y1 x2 y2 class\n",
        "            # Parsing only the first 8 elements as coordinates (ignoring class)\n",
        "            coordinates = [float(x) for x in line.strip().split()[:8]]\n",
        "            segmentation_info.append(coordinates)\n",
        "    return segmentation_info\n",
        "\n",
        "def crop_and_save(image_path, segmentation_info, output_folder):\n",
        "    \"\"\"\n",
        "    Crops the image based on the segmentation information and saves it to the output folder.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "    for i, seg_info in enumerate(segmentation_info):\n",
        "        # Extracting coordinates\n",
        "        x1, y1, x2, y2, x3, y3, x4, y4 = seg_info\n",
        "        # Finding minimum and maximum coordinates for the bounding box\n",
        "        min_x = min(x1, x2, x3, x4)\n",
        "        min_y = min(y1, y2, y3, y4)\n",
        "        max_x = max(x1, x2, x3, x4)\n",
        "        max_y = max(y1, y2, y3, y4)\n",
        "        # Cropping the region from the image\n",
        "        cropped_region = image.crop((min_x, min_y, max_x, max_y))\n",
        "        # Saving the cropped region to the output folder\n",
        "        cropped_region.save(os.path.join(output_folder, f\"{image_name}_crop_{i}.png\"))\n",
        "\n",
        "def process_dataset(image_folder, label_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Processes the dataset by reading images and their corresponding segmentation info,\n",
        "    cropping the images based on the segmentation info, and saving the cropped images\n",
        "    to the output folder.\n",
        "    \"\"\"\n",
        "    # Ensure the output folder exists, create if not\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # List all image files in the image folder\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Get the corresponding text file\n",
        "        txt_file = os.path.join(label_folder, image_file.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "        if os.path.exists(txt_file):\n",
        "            # Parse segmentation info from the text file\n",
        "            segmentation_info = parse_segmentation_info(txt_file)\n",
        "            # Crop and save the image based on segmentation info\n",
        "            crop_and_save(os.path.join(image_folder, image_file), segmentation_info, output_folder)\n",
        "\n",
        "# Example usage:\n",
        "image_folder = \"/content/gdrive/MyDrive/notebooks/train/images\"\n",
        "label_folder = \"/content/gdrive/MyDrive/notebooks/train/labelTxt\"\n",
        "output_folder = \"/content/gdrive/MyDrive/notebooks/crop\"\n",
        "\n",
        "process_dataset(image_folder, label_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "AI7j8Ud_tOEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def split_train_test_images(input_folder, train_folder, test_folder, test_ratio=0.2):\n",
        "    # Ensure the output folders exist, create if not\n",
        "    os.makedirs(train_folder, exist_ok=True)\n",
        "    os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "    # List all image files in the input folder\n",
        "    image_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
        "\n",
        "    # Shuffle the image files\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    # Calculate the number of images for the test set\n",
        "    num_test_images = int(len(image_files) * test_ratio)\n",
        "\n",
        "    # Split the image files into train and test sets\n",
        "    train_images = image_files[:-num_test_images]\n",
        "    test_images = image_files[-num_test_images:]\n",
        "\n",
        "    # Copy train images to the train folder\n",
        "    for image in train_images:\n",
        "        shutil.copyfile(os.path.join(input_folder, image), os.path.join(train_folder, image))\n",
        "\n",
        "    # Copy test images to the test folder\n",
        "    for image in test_images:\n",
        "        shutil.copyfile(os.path.join(input_folder, image), os.path.join(test_folder, image))\n",
        "\n",
        "# Paths\n",
        "input_folder = \"/content/gdrive/MyDrive/notebooks/crop\"\n",
        "train_folder = \"/content/gdrive/MyDrive/notebooks/trains\"\n",
        "test_folder = \"/content/gdrive/MyDrive/notebooks/tests\"\n",
        "\n",
        "# Split train and test images\n",
        "split_train_test_images(input_folder, train_folder, test_folder, test_ratio=0.2)\n"
      ],
      "metadata": {
        "id": "vuV2w9GlBYcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/ld-4') # 경로삭제"
      ],
      "metadata": {
        "id": "UFozJNcgvKMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def otsu_binarization(image):\n",
        "    \"\"\"\n",
        "    Applies OTSU binarization to the input image.\n",
        "    \"\"\"\n",
        "    # Convert image to numpy array\n",
        "    np_image = np.array(image)\n",
        "    # Convert image to grayscale\n",
        "    grayscale_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2GRAY)\n",
        "    # Apply OTSU thresholding to the grayscale image\n",
        "    _, binary_image = cv2.threshold(grayscale_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    return Image.fromarray(binary_image)\n",
        "\n",
        "def process_images_in_folder(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Processes all PNG images in the input folder using OTSU binarization\n",
        "    and saves the binarized images to the output folder.\n",
        "    \"\"\"\n",
        "    # Ensure the output folder exists, create if not\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # List all PNG files in the input folder\n",
        "    image_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Open and convert the image file to RGB mode\n",
        "        image_path = os.path.join(input_folder, image_file)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        # Apply OTSU binarization\n",
        "        binary_image = otsu_binarization(image)\n",
        "        # Save the binarized image to the output folder\n",
        "        output_path = os.path.join(output_folder, image_file)\n",
        "        binary_image.save(output_path)\n",
        "\n",
        "# Example usage:\n",
        "input_folder = \"/content/ld-4/train/train\"\n",
        "output_folder = \"/content/ld-4/train/train_otsu\"\n",
        "\n",
        "process_images_in_folder(input_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "D5Q0ZXwpuZhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall opencv-python-headless -y\n",
        "!pip uninstall opencv-contrib-python-headless -y\n",
        "!pip install opencv-python-headless\n",
        "!pip install opencv-contrib-python-headless\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "ge8Et0xzaCQ2",
        "outputId": "abaa6c7e-491c-4148-a04e-046e6d59ee73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opencv-python-headless 4.8.0.74\n",
            "Uninstalling opencv-python-headless-4.8.0.74:\n",
            "  Successfully uninstalled opencv-python-headless-4.8.0.74\n",
            "\u001b[33mWARNING: Skipping opencv-contrib-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n",
            "Installing collected packages: opencv-python-headless\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "roboflow 1.1.26 requires opencv-python-headless==4.8.0.74, but you have opencv-python-headless 4.9.0.80 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed opencv-python-headless-4.9.0.80\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              },
              "id": "85e039e4a0c44e53b02f04e76ece1abb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-contrib-python-headless\n",
            "  Downloading opencv_contrib_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (55.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python-headless) (1.25.2)\n",
            "Installing collected packages: opencv-contrib-python-headless\n",
            "Successfully installed opencv-contrib-python-headless-4.9.0.80\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              },
              "id": "4b7a05b57171483bb3b8b90a6234bc59"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from PIL import Image\n",
        "import shutil\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def create_autoencoder(input_shape):\n",
        "    input_img = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary_image = cv2.threshold(image.astype('uint8'), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    resized_image = cv2.resize(binary_image, (128, 128))\n",
        "    normalized_image = resized_image.astype('float32') / 255.0\n",
        "    final_image = np.reshape(normalized_image, (128, 128, 1))\n",
        "\n",
        "    return final_image\n",
        "\n",
        "# Function to train the Autoencoder\n",
        "def train_autoencoder(input_folder):\n",
        "\n",
        "    autoencoder = create_autoencoder((128, 128, 1))\n",
        "\n",
        "    # Load and preprocess images for training\n",
        "    image_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
        "    images = np.array([preprocess_image(f) for f in image_files])\n",
        "\n",
        "\n",
        "    checkpoint = ModelCheckpoint('autoencoder_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "    autoencoder.fit(images, images, epochs=200 , batch_size=32, shuffle=True, validation_split=0.2, callbacks=[checkpoint])\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "# Train the Autoencoder\n",
        "input_folder = \"/content/drive/MyDrive/notebooks/trains_arg\"\n",
        "autoencoder = train_autoencoder(input_folder)"
      ],
      "metadata": {
        "id": "xXC-6IKa6we_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6b1955-974c-4e2c-d646-3da2636e8aaf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2711\n",
            "Epoch 1: val_loss improved from inf to 0.19269, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 14s 46ms/step - loss: 0.2711 - val_loss: 0.1927\n",
            "Epoch 2/200\n",
            "  3/125 [..............................] - ETA: 3s - loss: 0.1947"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123/125 [============================>.] - ETA: 0s - loss: 0.1896\n",
            "Epoch 2: val_loss improved from 0.19269 to 0.18609, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.1897 - val_loss: 0.1861\n",
            "Epoch 3/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1799\n",
            "Epoch 3: val_loss improved from 0.18609 to 0.16999, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.1798 - val_loss: 0.1700\n",
            "Epoch 4/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1705\n",
            "Epoch 4: val_loss improved from 0.16999 to 0.16296, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.1704 - val_loss: 0.1630\n",
            "Epoch 5/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1660\n",
            "Epoch 5: val_loss improved from 0.16296 to 0.15918, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.1661 - val_loss: 0.1592\n",
            "Epoch 6/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1602\n",
            "Epoch 6: val_loss improved from 0.15918 to 0.15430, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.1604 - val_loss: 0.1543\n",
            "Epoch 7/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1575\n",
            "Epoch 7: val_loss improved from 0.15430 to 0.15333, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.1574 - val_loss: 0.1533\n",
            "Epoch 8/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1541\n",
            "Epoch 8: val_loss improved from 0.15333 to 0.14981, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1543 - val_loss: 0.1498\n",
            "Epoch 9/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1515\n",
            "Epoch 9: val_loss improved from 0.14981 to 0.14806, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1519 - val_loss: 0.1481\n",
            "Epoch 10/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1507\n",
            "Epoch 10: val_loss did not improve from 0.14806\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1508 - val_loss: 0.1482\n",
            "Epoch 11/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1485\n",
            "Epoch 11: val_loss improved from 0.14806 to 0.14376, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1484 - val_loss: 0.1438\n",
            "Epoch 12/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1460\n",
            "Epoch 12: val_loss improved from 0.14376 to 0.14331, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1461 - val_loss: 0.1433\n",
            "Epoch 13/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1445\n",
            "Epoch 13: val_loss improved from 0.14331 to 0.14108, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1445 - val_loss: 0.1411\n",
            "Epoch 14/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1439\n",
            "Epoch 14: val_loss did not improve from 0.14108\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1440 - val_loss: 0.1440\n",
            "Epoch 15/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1428\n",
            "Epoch 15: val_loss improved from 0.14108 to 0.13833, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1427 - val_loss: 0.1383\n",
            "Epoch 16/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1409\n",
            "Epoch 16: val_loss improved from 0.13833 to 0.13724, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.1409 - val_loss: 0.1372\n",
            "Epoch 17/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1400\n",
            "Epoch 17: val_loss improved from 0.13724 to 0.13713, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.1397 - val_loss: 0.1371\n",
            "Epoch 18/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1385\n",
            "Epoch 18: val_loss improved from 0.13713 to 0.13678, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.1385 - val_loss: 0.1368\n",
            "Epoch 19/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1382\n",
            "Epoch 19: val_loss improved from 0.13678 to 0.13562, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.1381 - val_loss: 0.1356\n",
            "Epoch 20/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1369\n",
            "Epoch 20: val_loss did not improve from 0.13562\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.1370 - val_loss: 0.1359\n",
            "Epoch 21/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1356\n",
            "Epoch 21: val_loss improved from 0.13562 to 0.13429, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.1357 - val_loss: 0.1343\n",
            "Epoch 22/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1347\n",
            "Epoch 22: val_loss improved from 0.13429 to 0.13189, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.1348 - val_loss: 0.1319\n",
            "Epoch 23/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1339\n",
            "Epoch 23: val_loss improved from 0.13189 to 0.13148, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1341 - val_loss: 0.1315\n",
            "Epoch 24/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1331\n",
            "Epoch 24: val_loss did not improve from 0.13148\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1331 - val_loss: 0.1356\n",
            "Epoch 25/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1331\n",
            "Epoch 25: val_loss improved from 0.13148 to 0.13062, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1329 - val_loss: 0.1306\n",
            "Epoch 26/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1322\n",
            "Epoch 26: val_loss did not improve from 0.13062\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1321 - val_loss: 0.1343\n",
            "Epoch 27/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1313\n",
            "Epoch 27: val_loss improved from 0.13062 to 0.12820, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1313 - val_loss: 0.1282\n",
            "Epoch 28/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1308\n",
            "Epoch 28: val_loss improved from 0.12820 to 0.12786, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1306 - val_loss: 0.1279\n",
            "Epoch 29/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1302\n",
            "Epoch 29: val_loss did not improve from 0.12786\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1302 - val_loss: 0.1281\n",
            "Epoch 30/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1296\n",
            "Epoch 30: val_loss improved from 0.12786 to 0.12633, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1296 - val_loss: 0.1263\n",
            "Epoch 31/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1289\n",
            "Epoch 31: val_loss did not improve from 0.12633\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1289 - val_loss: 0.1319\n",
            "Epoch 32/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1290\n",
            "Epoch 32: val_loss improved from 0.12633 to 0.12571, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1291 - val_loss: 0.1257\n",
            "Epoch 33/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1275\n",
            "Epoch 33: val_loss did not improve from 0.12571\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1279 - val_loss: 0.1289\n",
            "Epoch 34/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1278\n",
            "Epoch 34: val_loss improved from 0.12571 to 0.12488, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1279 - val_loss: 0.1249\n",
            "Epoch 35/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1267\n",
            "Epoch 35: val_loss did not improve from 0.12488\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1269 - val_loss: 0.1256\n",
            "Epoch 36/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1272\n",
            "Epoch 36: val_loss improved from 0.12488 to 0.12363, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1270 - val_loss: 0.1236\n",
            "Epoch 37/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1266\n",
            "Epoch 37: val_loss did not improve from 0.12363\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1265 - val_loss: 0.1238\n",
            "Epoch 38/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1258\n",
            "Epoch 38: val_loss did not improve from 0.12363\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1257 - val_loss: 0.1264\n",
            "Epoch 39/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1259\n",
            "Epoch 39: val_loss did not improve from 0.12363\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1259 - val_loss: 0.1392\n",
            "Epoch 40/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1259\n",
            "Epoch 40: val_loss improved from 0.12363 to 0.12339, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1260 - val_loss: 0.1234\n",
            "Epoch 41/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1245\n",
            "Epoch 41: val_loss improved from 0.12339 to 0.12220, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1246 - val_loss: 0.1222\n",
            "Epoch 42/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1242\n",
            "Epoch 42: val_loss did not improve from 0.12220\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1243 - val_loss: 0.1226\n",
            "Epoch 43/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1245\n",
            "Epoch 43: val_loss did not improve from 0.12220\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1245 - val_loss: 0.1238\n",
            "Epoch 44/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1238\n",
            "Epoch 44: val_loss improved from 0.12220 to 0.12155, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1238 - val_loss: 0.1216\n",
            "Epoch 45/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1234\n",
            "Epoch 45: val_loss did not improve from 0.12155\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1236 - val_loss: 0.1219\n",
            "Epoch 46/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1234\n",
            "Epoch 46: val_loss improved from 0.12155 to 0.12076, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1232 - val_loss: 0.1208\n",
            "Epoch 47/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1230\n",
            "Epoch 47: val_loss did not improve from 0.12076\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1230 - val_loss: 0.1214\n",
            "Epoch 48/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1230\n",
            "Epoch 48: val_loss improved from 0.12076 to 0.12070, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1230 - val_loss: 0.1207\n",
            "Epoch 49/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1222\n",
            "Epoch 49: val_loss improved from 0.12070 to 0.12014, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1224 - val_loss: 0.1201\n",
            "Epoch 50/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1229\n",
            "Epoch 50: val_loss improved from 0.12014 to 0.11975, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1228 - val_loss: 0.1197\n",
            "Epoch 51/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1215\n",
            "Epoch 51: val_loss improved from 0.11975 to 0.11950, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1217 - val_loss: 0.1195\n",
            "Epoch 52/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1216\n",
            "Epoch 52: val_loss did not improve from 0.11950\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1217 - val_loss: 0.1221\n",
            "Epoch 53/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1215\n",
            "Epoch 53: val_loss did not improve from 0.11950\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1216 - val_loss: 0.1195\n",
            "Epoch 54/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1214\n",
            "Epoch 54: val_loss improved from 0.11950 to 0.11906, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1215 - val_loss: 0.1191\n",
            "Epoch 55/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1210\n",
            "Epoch 55: val_loss did not improve from 0.11906\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1209 - val_loss: 0.1197\n",
            "Epoch 56/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1209\n",
            "Epoch 56: val_loss did not improve from 0.11906\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1208 - val_loss: 0.1205\n",
            "Epoch 57/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1207\n",
            "Epoch 57: val_loss improved from 0.11906 to 0.11805, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1205 - val_loss: 0.1181\n",
            "Epoch 58/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1207\n",
            "Epoch 58: val_loss did not improve from 0.11805\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1205 - val_loss: 0.1182\n",
            "Epoch 59/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1201\n",
            "Epoch 59: val_loss did not improve from 0.11805\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1200 - val_loss: 0.1222\n",
            "Epoch 60/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1198\n",
            "Epoch 60: val_loss did not improve from 0.11805\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1200 - val_loss: 0.1379\n",
            "Epoch 61/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1211\n",
            "Epoch 61: val_loss did not improve from 0.11805\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1211 - val_loss: 0.1205\n",
            "Epoch 62/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1191\n",
            "Epoch 62: val_loss did not improve from 0.11805\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1194 - val_loss: 0.1194\n",
            "Epoch 63/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1190\n",
            "Epoch 63: val_loss improved from 0.11805 to 0.11771, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1191 - val_loss: 0.1177\n",
            "Epoch 64/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1196\n",
            "Epoch 64: val_loss did not improve from 0.11771\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1196 - val_loss: 0.1200\n",
            "Epoch 65/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1199\n",
            "Epoch 65: val_loss improved from 0.11771 to 0.11688, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1195 - val_loss: 0.1169\n",
            "Epoch 66/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1188\n",
            "Epoch 66: val_loss did not improve from 0.11688\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1188 - val_loss: 0.1174\n",
            "Epoch 67/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1185\n",
            "Epoch 67: val_loss did not improve from 0.11688\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1185 - val_loss: 0.1198\n",
            "Epoch 68/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1193\n",
            "Epoch 68: val_loss did not improve from 0.11688\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1191 - val_loss: 0.1183\n",
            "Epoch 69/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1186\n",
            "Epoch 69: val_loss did not improve from 0.11688\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1184 - val_loss: 0.1170\n",
            "Epoch 70/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1184\n",
            "Epoch 70: val_loss improved from 0.11688 to 0.11645, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1185 - val_loss: 0.1165\n",
            "Epoch 71/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1177\n",
            "Epoch 71: val_loss did not improve from 0.11645\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1179 - val_loss: 0.1184\n",
            "Epoch 72/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1183\n",
            "Epoch 72: val_loss improved from 0.11645 to 0.11595, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1183 - val_loss: 0.1159\n",
            "Epoch 73/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1181\n",
            "Epoch 73: val_loss did not improve from 0.11595\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1180 - val_loss: 0.1163\n",
            "Epoch 74/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1176\n",
            "Epoch 74: val_loss did not improve from 0.11595\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1177 - val_loss: 0.1160\n",
            "Epoch 75/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1177\n",
            "Epoch 75: val_loss did not improve from 0.11595\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1176 - val_loss: 0.1173\n",
            "Epoch 76/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1178\n",
            "Epoch 76: val_loss did not improve from 0.11595\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1178 - val_loss: 0.1169\n",
            "Epoch 77/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1176\n",
            "Epoch 77: val_loss did not improve from 0.11595\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1177 - val_loss: 0.1169\n",
            "Epoch 78/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1172\n",
            "Epoch 78: val_loss did not improve from 0.11595\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1171 - val_loss: 0.1172\n",
            "Epoch 79/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1173\n",
            "Epoch 79: val_loss did not improve from 0.11595\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1173 - val_loss: 0.1192\n",
            "Epoch 80/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1179\n",
            "Epoch 80: val_loss did not improve from 0.11595\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1178 - val_loss: 0.1160\n",
            "Epoch 81/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1172\n",
            "Epoch 81: val_loss improved from 0.11595 to 0.11516, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1173 - val_loss: 0.1152\n",
            "Epoch 82/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1166\n",
            "Epoch 82: val_loss did not improve from 0.11516\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1165 - val_loss: 0.1177\n",
            "Epoch 83/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1169\n",
            "Epoch 83: val_loss improved from 0.11516 to 0.11486, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1169 - val_loss: 0.1149\n",
            "Epoch 84/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1165\n",
            "Epoch 84: val_loss improved from 0.11486 to 0.11481, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1162 - val_loss: 0.1148\n",
            "Epoch 85/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1166\n",
            "Epoch 85: val_loss did not improve from 0.11481\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1165 - val_loss: 0.1151\n",
            "Epoch 86/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1163\n",
            "Epoch 86: val_loss did not improve from 0.11481\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1162 - val_loss: 0.1158\n",
            "Epoch 87/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1165\n",
            "Epoch 87: val_loss did not improve from 0.11481\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1163 - val_loss: 0.1171\n",
            "Epoch 88/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1164\n",
            "Epoch 88: val_loss did not improve from 0.11481\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1164 - val_loss: 0.1150\n",
            "Epoch 89/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1157\n",
            "Epoch 89: val_loss did not improve from 0.11481\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1159 - val_loss: 0.1265\n",
            "Epoch 90/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1164\n",
            "Epoch 90: val_loss did not improve from 0.11481\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1162 - val_loss: 0.1164\n",
            "Epoch 91/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1159\n",
            "Epoch 91: val_loss improved from 0.11481 to 0.11458, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1157 - val_loss: 0.1146\n",
            "Epoch 92/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1160\n",
            "Epoch 92: val_loss did not improve from 0.11458\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1158 - val_loss: 0.1156\n",
            "Epoch 93/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1152\n",
            "Epoch 93: val_loss improved from 0.11458 to 0.11385, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1152 - val_loss: 0.1138\n",
            "Epoch 94/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1154\n",
            "Epoch 94: val_loss did not improve from 0.11385\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1154 - val_loss: 0.1142\n",
            "Epoch 95/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1155\n",
            "Epoch 95: val_loss did not improve from 0.11385\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1155 - val_loss: 0.1140\n",
            "Epoch 96/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1155\n",
            "Epoch 96: val_loss did not improve from 0.11385\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1153 - val_loss: 0.1139\n",
            "Epoch 97/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1151\n",
            "Epoch 97: val_loss did not improve from 0.11385\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1150 - val_loss: 0.1155\n",
            "Epoch 98/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1151\n",
            "Epoch 98: val_loss improved from 0.11385 to 0.11364, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1151 - val_loss: 0.1136\n",
            "Epoch 99/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1148\n",
            "Epoch 99: val_loss did not improve from 0.11364\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1149 - val_loss: 0.1149\n",
            "Epoch 100/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1149\n",
            "Epoch 100: val_loss did not improve from 0.11364\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1150 - val_loss: 0.1140\n",
            "Epoch 101/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1150\n",
            "Epoch 101: val_loss improved from 0.11364 to 0.11313, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1149 - val_loss: 0.1131\n",
            "Epoch 102/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1146\n",
            "Epoch 102: val_loss did not improve from 0.11313\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1145 - val_loss: 0.1152\n",
            "Epoch 103/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1142\n",
            "Epoch 103: val_loss did not improve from 0.11313\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1142 - val_loss: 0.1152\n",
            "Epoch 104/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1145\n",
            "Epoch 104: val_loss did not improve from 0.11313\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1144 - val_loss: 0.1140\n",
            "Epoch 105/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1145\n",
            "Epoch 105: val_loss did not improve from 0.11313\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1143 - val_loss: 0.1151\n",
            "Epoch 106/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1145\n",
            "Epoch 106: val_loss did not improve from 0.11313\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1145 - val_loss: 0.1146\n",
            "Epoch 107/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1143\n",
            "Epoch 107: val_loss improved from 0.11313 to 0.11297, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1143 - val_loss: 0.1130\n",
            "Epoch 108/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1139\n",
            "Epoch 108: val_loss did not improve from 0.11297\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1139 - val_loss: 0.1139\n",
            "Epoch 109/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1142\n",
            "Epoch 109: val_loss did not improve from 0.11297\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1142 - val_loss: 0.1134\n",
            "Epoch 110/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1138\n",
            "Epoch 110: val_loss did not improve from 0.11297\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1139 - val_loss: 0.1130\n",
            "Epoch 111/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1138\n",
            "Epoch 111: val_loss did not improve from 0.11297\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1138 - val_loss: 0.1133\n",
            "Epoch 112/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1147\n",
            "Epoch 112: val_loss did not improve from 0.11297\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1146 - val_loss: 0.1130\n",
            "Epoch 113/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1133\n",
            "Epoch 113: val_loss did not improve from 0.11297\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1136 - val_loss: 0.1160\n",
            "Epoch 114/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1136\n",
            "Epoch 114: val_loss did not improve from 0.11297\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1137 - val_loss: 0.1156\n",
            "Epoch 115/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1136\n",
            "Epoch 115: val_loss improved from 0.11297 to 0.11233, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1137 - val_loss: 0.1123\n",
            "Epoch 116/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1141\n",
            "Epoch 116: val_loss did not improve from 0.11233\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1141 - val_loss: 0.1133\n",
            "Epoch 117/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1138\n",
            "Epoch 117: val_loss did not improve from 0.11233\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1137 - val_loss: 0.1128\n",
            "Epoch 118/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1129\n",
            "Epoch 118: val_loss did not improve from 0.11233\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1130 - val_loss: 0.1134\n",
            "Epoch 119/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1132\n",
            "Epoch 119: val_loss improved from 0.11233 to 0.11228, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1134 - val_loss: 0.1123\n",
            "Epoch 120/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1128\n",
            "Epoch 120: val_loss did not improve from 0.11228\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1130 - val_loss: 0.1125\n",
            "Epoch 121/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1129\n",
            "Epoch 121: val_loss did not improve from 0.11228\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1128 - val_loss: 0.1147\n",
            "Epoch 122/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1133\n",
            "Epoch 122: val_loss did not improve from 0.11228\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1133 - val_loss: 0.1143\n",
            "Epoch 123/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1128\n",
            "Epoch 123: val_loss did not improve from 0.11228\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1128 - val_loss: 0.1124\n",
            "Epoch 124/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1132\n",
            "Epoch 124: val_loss did not improve from 0.11228\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1131 - val_loss: 0.1124\n",
            "Epoch 125/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1126\n",
            "Epoch 125: val_loss did not improve from 0.11228\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1126 - val_loss: 0.1164\n",
            "Epoch 126/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1127\n",
            "Epoch 126: val_loss improved from 0.11228 to 0.11193, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1129 - val_loss: 0.1119\n",
            "Epoch 127/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1131\n",
            "Epoch 127: val_loss did not improve from 0.11193\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1133 - val_loss: 0.1133\n",
            "Epoch 128/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1129\n",
            "Epoch 128: val_loss did not improve from 0.11193\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1129 - val_loss: 0.1159\n",
            "Epoch 129/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1127\n",
            "Epoch 129: val_loss improved from 0.11193 to 0.11177, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1127 - val_loss: 0.1118\n",
            "Epoch 130/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1121\n",
            "Epoch 130: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1123 - val_loss: 0.1119\n",
            "Epoch 131/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1124\n",
            "Epoch 131: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1123 - val_loss: 0.1142\n",
            "Epoch 132/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1127\n",
            "Epoch 132: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1125 - val_loss: 0.1223\n",
            "Epoch 133/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1131\n",
            "Epoch 133: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1130 - val_loss: 0.1121\n",
            "Epoch 134/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1121\n",
            "Epoch 134: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1122 - val_loss: 0.1118\n",
            "Epoch 135/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1124\n",
            "Epoch 135: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1124 - val_loss: 0.1124\n",
            "Epoch 136/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1124\n",
            "Epoch 136: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1123 - val_loss: 0.1147\n",
            "Epoch 137/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1121\n",
            "Epoch 137: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1122 - val_loss: 0.1121\n",
            "Epoch 138/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1122\n",
            "Epoch 138: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1123 - val_loss: 0.1119\n",
            "Epoch 139/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1119\n",
            "Epoch 139: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1119 - val_loss: 0.1123\n",
            "Epoch 140/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1119\n",
            "Epoch 140: val_loss did not improve from 0.11177\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1118 - val_loss: 0.1138\n",
            "Epoch 141/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1127\n",
            "Epoch 141: val_loss improved from 0.11177 to 0.11173, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1128 - val_loss: 0.1117\n",
            "Epoch 142/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1118\n",
            "Epoch 142: val_loss did not improve from 0.11173\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1117 - val_loss: 0.1131\n",
            "Epoch 143/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1119\n",
            "Epoch 143: val_loss improved from 0.11173 to 0.11130, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1117 - val_loss: 0.1113\n",
            "Epoch 144/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1117\n",
            "Epoch 144: val_loss did not improve from 0.11130\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1118 - val_loss: 0.1114\n",
            "Epoch 145/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1115\n",
            "Epoch 145: val_loss did not improve from 0.11130\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1117 - val_loss: 0.1113\n",
            "Epoch 146/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1116\n",
            "Epoch 146: val_loss improved from 0.11130 to 0.11126, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1117 - val_loss: 0.1113\n",
            "Epoch 147/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1116\n",
            "Epoch 147: val_loss improved from 0.11126 to 0.11121, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1116 - val_loss: 0.1112\n",
            "Epoch 148/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1115\n",
            "Epoch 148: val_loss did not improve from 0.11121\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1116 - val_loss: 0.1171\n",
            "Epoch 149/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1115\n",
            "Epoch 149: val_loss did not improve from 0.11121\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1118 - val_loss: 0.1128\n",
            "Epoch 150/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1114\n",
            "Epoch 150: val_loss did not improve from 0.11121\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1112 - val_loss: 0.1112\n",
            "Epoch 151/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1114\n",
            "Epoch 151: val_loss improved from 0.11121 to 0.11063, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1114 - val_loss: 0.1106\n",
            "Epoch 152/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1112\n",
            "Epoch 152: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1113 - val_loss: 0.1112\n",
            "Epoch 153/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1110\n",
            "Epoch 153: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1111 - val_loss: 0.1113\n",
            "Epoch 154/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1113\n",
            "Epoch 154: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1112 - val_loss: 0.1114\n",
            "Epoch 155/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1118\n",
            "Epoch 155: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1118 - val_loss: 0.1109\n",
            "Epoch 156/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1111\n",
            "Epoch 156: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1112 - val_loss: 0.1116\n",
            "Epoch 157/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1112\n",
            "Epoch 157: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1111 - val_loss: 0.1109\n",
            "Epoch 158/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1111\n",
            "Epoch 158: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1112 - val_loss: 0.1108\n",
            "Epoch 159/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1114\n",
            "Epoch 159: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1115 - val_loss: 0.1131\n",
            "Epoch 160/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1109\n",
            "Epoch 160: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1109 - val_loss: 0.1132\n",
            "Epoch 161/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1119\n",
            "Epoch 161: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1118 - val_loss: 0.1112\n",
            "Epoch 162/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1103\n",
            "Epoch 162: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1105 - val_loss: 0.1114\n",
            "Epoch 163/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1106\n",
            "Epoch 163: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1108 - val_loss: 0.1118\n",
            "Epoch 164/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1109\n",
            "Epoch 164: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1109 - val_loss: 0.1117\n",
            "Epoch 165/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1104\n",
            "Epoch 165: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1106 - val_loss: 0.1119\n",
            "Epoch 166/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1115\n",
            "Epoch 166: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1114 - val_loss: 0.1109\n",
            "Epoch 167/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1104\n",
            "Epoch 167: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1104 - val_loss: 0.1110\n",
            "Epoch 168/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1105\n",
            "Epoch 168: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1104 - val_loss: 0.1139\n",
            "Epoch 169/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1114\n",
            "Epoch 169: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1112 - val_loss: 0.1112\n",
            "Epoch 170/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1104\n",
            "Epoch 170: val_loss did not improve from 0.11063\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1105 - val_loss: 0.1122\n",
            "Epoch 171/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1103\n",
            "Epoch 171: val_loss improved from 0.11063 to 0.11047, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1105 - val_loss: 0.1105\n",
            "Epoch 172/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1106\n",
            "Epoch 172: val_loss improved from 0.11047 to 0.11043, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1105 - val_loss: 0.1104\n",
            "Epoch 173/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1106\n",
            "Epoch 173: val_loss did not improve from 0.11043\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1107 - val_loss: 0.1138\n",
            "Epoch 174/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1110\n",
            "Epoch 174: val_loss did not improve from 0.11043\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1111 - val_loss: 0.1110\n",
            "Epoch 175/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1101\n",
            "Epoch 175: val_loss did not improve from 0.11043\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1104 - val_loss: 0.1109\n",
            "Epoch 176/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1103\n",
            "Epoch 176: val_loss did not improve from 0.11043\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1101 - val_loss: 0.1110\n",
            "Epoch 177/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1106\n",
            "Epoch 177: val_loss improved from 0.11043 to 0.11034, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1104 - val_loss: 0.1103\n",
            "Epoch 178/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1099\n",
            "Epoch 178: val_loss did not improve from 0.11034\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1100 - val_loss: 0.1110\n",
            "Epoch 179/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1103\n",
            "Epoch 179: val_loss improved from 0.11034 to 0.11016, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1102 - val_loss: 0.1102\n",
            "Epoch 180/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1098\n",
            "Epoch 180: val_loss did not improve from 0.11016\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1099 - val_loss: 0.1111\n",
            "Epoch 181/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1103\n",
            "Epoch 181: val_loss did not improve from 0.11016\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1105 - val_loss: 0.1105\n",
            "Epoch 182/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1100\n",
            "Epoch 182: val_loss improved from 0.11016 to 0.10998, saving model to autoencoder_weights.h5\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1102 - val_loss: 0.1100\n",
            "Epoch 183/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1098\n",
            "Epoch 183: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1098 - val_loss: 0.1125\n",
            "Epoch 184/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1100\n",
            "Epoch 184: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1101 - val_loss: 0.1117\n",
            "Epoch 185/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1097\n",
            "Epoch 185: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1098 - val_loss: 0.1119\n",
            "Epoch 186/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1106\n",
            "Epoch 186: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1105 - val_loss: 0.1125\n",
            "Epoch 187/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1099\n",
            "Epoch 187: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1100 - val_loss: 0.1109\n",
            "Epoch 188/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1096\n",
            "Epoch 188: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1096 - val_loss: 0.1159\n",
            "Epoch 189/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1095\n",
            "Epoch 189: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1097 - val_loss: 0.1126\n",
            "Epoch 190/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1095\n",
            "Epoch 190: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1098 - val_loss: 0.1100\n",
            "Epoch 191/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1097\n",
            "Epoch 191: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1096 - val_loss: 0.1116\n",
            "Epoch 192/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1101\n",
            "Epoch 192: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1101 - val_loss: 0.1111\n",
            "Epoch 193/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1096\n",
            "Epoch 193: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1098 - val_loss: 0.1113\n",
            "Epoch 194/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1094\n",
            "Epoch 194: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1095 - val_loss: 0.1108\n",
            "Epoch 195/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1099\n",
            "Epoch 195: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1098 - val_loss: 0.1112\n",
            "Epoch 196/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1096\n",
            "Epoch 196: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1098 - val_loss: 0.1121\n",
            "Epoch 197/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1097\n",
            "Epoch 197: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1097 - val_loss: 0.1131\n",
            "Epoch 198/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1096\n",
            "Epoch 198: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1094 - val_loss: 0.1116\n",
            "Epoch 199/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1096\n",
            "Epoch 199: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1095 - val_loss: 0.1177\n",
            "Epoch 200/200\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1107\n",
            "Epoch 200: val_loss did not improve from 0.10998\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1105 - val_loss: 0.1109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect anomalies\n",
        "def detect_anomalies(autoencoder, image_path, threshold=0.01):\n",
        "    # Preprocess input image\n",
        "    input_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Convert to uint8\n",
        "    input_image = input_image.astype('uint8')\n",
        "    # Perform OTSU thresholding\n",
        "    _, binary_image = cv2.threshold(input_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    resized_image = cv2.resize(binary_image, (128, 128))\n",
        "    normalized_image = resized_image.astype('float32') / 255.0\n",
        "    final_image = np.reshape(normalized_image, (1, 128, 128, 1))\n",
        "\n",
        "    # Generate reconstruction from Autoencoder\n",
        "    reconstructed_image = autoencoder.predict(final_image)\n",
        "    # Calculate Mean Squared Error (MSE) between input and reconstructed images\n",
        "    mse = np.mean(np.square(final_image - reconstructed_image))\n",
        "\n",
        "    # Determine anomaly based on MSE and threshold\n",
        "    if mse > threshold:\n",
        "        return 1  # Anomaly\n",
        "    else:\n",
        "        return 0  # Normal\n",
        "\n",
        "\n",
        "# Function to evaluate anomalies in a test folder\n",
        "def evaluate_anomalies(autoencoder, test_folder):\n",
        "    labels = []\n",
        "    predictions = []\n",
        "\n",
        "    for subdir in os.listdir(test_folder):\n",
        "        subdir_path = os.path.join(test_folder, subdir)\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            continue\n",
        "\n",
        "        for image_file in os.listdir(subdir_path):\n",
        "            image_path = os.path.join(subdir_path, image_file)\n",
        "            label = int(subdir)  # Use folder name as label\n",
        "            labels.append(label)\n",
        "            prediction = detect_anomalies(autoencoder, image_path)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "    return np.array(labels), np.array(predictions)\n",
        "\n",
        "# Evaluate anomalies in the test folder\n",
        "test_folder = \"/content/drive/MyDrive/notebooks/tests\"\n",
        "true_labels, predicted_labels = evaluate_anomalies(autoencoder, test_folder)\n",
        "\n",
        "# Calculate F1 score and confusion matrix\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2BddIhmcp_J",
        "outputId": "82495c2f-e9f2-4341-faaa-710b20477925"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "F1 Score: 0.16623376623376623\n",
            "Confusion Matrix:\n",
            "[[137 320]\n",
            " [  1  32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import shutil\n",
        "# 원본 이미지가 있는 폴더 경로\n",
        "original_folder = \"/content/drive/MyDrive/notebooks/trains\"\n",
        "# 증강된 이미지를 저장할 폴더 경로\n",
        "augmented_folder = \"/content/drive/MyDrive/notebooks/trains_arg\"\n",
        "\n",
        "# 폴더가 없으면 생성\n",
        "if not os.path.exists(augmented_folder):\n",
        "    os.makedirs(augmented_folder)\n",
        "\n",
        "# 이미지 데이터 생성기 생성\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# 이미지 파일 목록 가져오기\n",
        "image_files = [os.path.join(original_folder, f)\n",
        "               for f in os.listdir(original_folder)\n",
        "               if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
        "\n",
        "# 증강된 이미지 생성 및 저장\n",
        "for image_file in image_files:\n",
        "    # 이미지 로드\n",
        "    image = cv2.imread(image_file)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = np.expand_dims(image, axis=0)  # 배치 차원 추가\n",
        "\n",
        "    # 증강된 이미지 생성\n",
        "    iterator = datagen.flow(image, batch_size=1, save_to_dir=augmented_folder, save_prefix='augmented_', save_format='png')\n",
        "    for i in range(50):  # 각 이미지 당 50개의 증강된 이미지 생성\n",
        "        next(iterator)\n"
      ],
      "metadata": {
        "id": "ZYLmtUA5aNHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 이미지를 증강된 이미지 폴더로 복사\n",
        "for image_file in image_files:\n",
        "    filename = os.path.basename(image_file)\n",
        "    original_path = os.path.join(original_folder, filename)\n",
        "    augmented_path = os.path.join(augmented_folder, filename)\n",
        "    shutil.copyfile(original_path, augmented_path)\n"
      ],
      "metadata": {
        "id": "84O-xPWwLwuA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}